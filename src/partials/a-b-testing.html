{{#markdown}}

{{ name }} 
======
 ** Find our which designs interventions actually work, by testing their efficacy 
 in a live environment using hypothesis testing methods.
 **

 It can help to answer:
 * Which design helps users reach their desired goals most efficiently?
 * Which design helps to generates the most profit?
 * Which design intervention engages the user for the longest?
Effectively this research method can be used to compare the actual performance 
of one design over another using statistical methods to validate.
 

## In a Nutshell
A/B tests are scientifically controlled experiments. Multivariate tests simply refers 
to there being more than one variation being tested.

A very simple A/B test could be to identify the most effective buy button design on 
a product page. The goal would be to identify whether the new button design prompted 
more users to click on it, than the existing one.

To ensure a fair test define the success criteria early on when designing the test.

An A/B testing tool such as Adobe Target, Google Optimize,  Optimizely, Unbounce or 
Maxymiser, etc. would be used in web/app environments, where the alternate designs 
for the test could be setup.

The researcher then configures the test to allow a percentage of random traffic to 
reach the variant designs over an agreed period. This allows for enough data to be 
collected for the sample to yield a statistically significant result.

Once complete the data would be analysed and the appropriate model used to find a 
statistically significant result. 


## When is it Typically Used?
This is used on live products, once they have actual real-world usage with your 
user population. 


## Measures
Any number of measures can be used to in an  A/B test, these could be as simple as 
click-throughs to a more complex combination of ratios. Measures along with confidence 
intervals and success/failure thresholds must be defined in the initial test design.


## Real World Tips
  * Keep tests simple in design, where possible minimise test variables to understand 
  what is/isn't working.
  * A/B tests can be run all the time, have a stack of tests going to keep improving 
  the application performance.
  * Utilise the correct statistical methods to ensure test results are accurate.
  * Avoid testing too many variants with multiple variables, as it becomes more difficult 
  to ascertain why a particular design is more successful over the other variants.
  
  
## Positives
  * Cheap and effective to run. 
  * Numerous tests can be run simultaneously along non-intersecting site/application journeys.
  * Allows for incremental improvements to be made on an experience, to improve it's throughput. 
  * Very effective tool in refining goal based journeys i.e. purchase paths, shopping carts. 
  * De-risks large changes by getting real insights from a sample group, before releasing 
  to the entire audience.
  

## Drawbacks
  * Provides shallow insights - does not tell you why users are performing said behaviours - 
  a full suite of usability testing delivers deeper behavioural insight.
  * Only really useful when focusing on a single goal, and success metric
  * It's narrow focus can side-line other valuable non-quantifiable aspects of a design - 
  brand, trust, security, perception and such at the cost of achieving a particular goal.
  * Does not give you insights into what aspects of your interventions are effective.
  * Just because an intervention succeeds does not mean it is actually effective, it's only 
  better than the other tested version(s).
  * A/B tests are usually deliver short-term results - not longer term strategic goals.
  

## Example Scenario
A/B testing can be used to determine which of a number of landing page designs for a digital 
magazine subscription, yields the highest conversion rate into actual sales. 

Inbound visitors via search ads would be randomly directed to one of a series of landing pages, 
over a set period of time and benchmarked against an effective control (usually the existing version). 
Enough data would need to be collected to determine a statistically significant result. 

The one which yields the highest conversion into sales would be deemed the most effective with 
regards to sale conversion. 


## Useful Resources - Links
* <a href="https://www.gov.uk/guidance/ab-testing-comparative-studies" rel="noopener" target="_blank">
  UK GOV: A/B testing: comparative studies <span class="show-for-sr">(Opens in a new window)</span></a>
* <a href="https://www.optimizely.com/uk/optimization-glossary/ab-testing/" rel="noopener" target="_blank">
  Optimizely - A/B Testing <span class="show-for-sr">(Opens in a new window)</span></a>
* <a href="https://www.nngroup.com/articles/putting-ab-testing-in-its-place/" rel="noopener" target="_blank">
  Putting A/B Testing in Its Place <span class="show-for-sr">(Opens in a new window)</span></a>
* <a href="https://www.nngroup.com/articles/risks-of-quantitative-studies/" rel="noopener" target="_blank">
  Risks of Quantitative Studies <span class="show-for-sr">(Opens in a new window)</span></a>

{{/markdown}}
