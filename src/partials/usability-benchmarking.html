{{#markdown}}

{{ name }} 
======
 **Get ongoing feedback around the usability performance and satisfaction 
 of a system relative to a baseline (previous version or competitor product) over time.
 **

It helps to answer questions such as:
 * Has the usability of my product improved?
 * Which areas of my product need attention?
 * How does my product compare against a competitors'?


## In a Nutshell
To usability benchmark the product, the researcher needs to create a clear testing script 
that has goal based tasks. Following each task is an evaluative question that can be used 
to measure task satisfaction for the user.

Benchmarking across product iterations, would mean initially testing on the existing product, 
then performing the same test again at a later time when the product had been updated with 
improvements, with the same size cohort. 

Benchmarking would typically be done at regular intervals  (i.e. quarterly) - data would be 
compared with previous iterations to identify trends.

Usability benchmarking can be done in a lab or remotely and unmoderated - to get a larger number 
of participants for a quantitative study, it's usually is easier to conduct testing remotely and unmoderated.

As we're trying to capture mote quantitative insights, cohort sizes will be larger (20 - 50 users) 
than a typical usability study. Cohort sizes need to be consistent across the tests.


## When is it Typically Used?
This is a summative test, and typically run at regular intervals after improvements to the 
product have been made.

This could be on a fortnightly basis if improvements are more frequent or a quarterly 
cycle if releases are infrequent.


## Measures
Generally a qualitative study, but with larger cohorts numerical data can be used to draw trends.

Performance based metrics to capture, include:

 | Measure           | Details                                |
 | ----------------- |:---------------------------------------| 
 | **Task success**  | Full, Partial, Incomplete successes    |
 | **Time on task**  | Time taken to complete task            |
 | **Task failure**  | If the task was unsuccessful           |

Behavioural metrics to elicit, include:
 * Ease/difficulty of task
 * If outcomes were as expected
 * Differential in expected time on task (should task have taken longer or less time)


## Real World Tips
  * Run test remotely with a good script to capture more users quickly. 
  * Analyse the data carefully to identify task completion and failure rates.
  * Identify the difference between the perception of expected task completion time vs actual completion time.
  * Use remote testing tools that capture rich data such as clickstreams and screen recordings.
  * Some tools can also record live video of the test - this can be useful to capture facial expressions.
  * For capturing behavioural feedback - use a Likert or semantic differential scale for capturing answers.


## Positives
  * Shows if a product is improving over time, which helps to inform future product strategies.
  * Highlights if users are learning how to use the system.
  * Can compare benchmarks against industry standards and competitors.


## Drawbacks
  * To capture comparable numeric data, cohort sizes need to be larger and can make testing time consuming and costly to run.
  * Shifts in the testing script need to be managed carefully so as to ensure comparisons can remain meaningful.


## Example Scenario
Usability benchmarking a TV set-top-box interface would inform product developers about how 
much the product is improving over time, using a combination of performance and perception based metrics


## Useful Resources - Links
* <a href="https://measuringu.com/benchmark-intro/" rel="noopener" target="_blank">
    An introduction to usability benchmarking <span class="show-for-sr">(Opens in a new window)</span></a>
* <a href="https://www.gov.uk/service-manual/measuring-success/usability-benchmarking-a-website-or-whole-service" rel="noopener" target="_blank">
    Usability benchmarking a website or whole service <span class="show-for-sr">(Opens in a new window)</span></a>

{{/markdown}}
